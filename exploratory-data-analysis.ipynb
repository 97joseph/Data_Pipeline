{"metadata":{"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"EXPLORATORY DATA ANALYSIS WITH PYTHON","metadata":{}},{"cell_type":"markdown","source":"Exploratory Data Analysis (EDA) is an open-ended process where we calculate statistics and make figures to find trends, anomalies, patterns, or relationships within the data. The goal of EDA is to learn what our data can tell us. It generally starts out with a high level overview, then narrows in to specific areas as we find intriguing areas of the data. The findings may be interesting in their own right, or they can be used to inform our modeling choices, such as by helping us decide which features to use.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Comment this if the data visualisations doesn't work on your side\n%matplotlib inline\n\nplt.style.use('bmh')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weatherdata = pd.read_csv('../input/../input/environment/clean_weatherdata.csv')\nweatherdata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"airportdata = pd.read_csv('../input/../input/../input/environment/Airport_car.csv')\nairportdata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"Examine Missing Values\nNext we can look at the number and percentage of missing values in each column.","metadata":{}},{"cell_type":"markdown","source":"WEATHER DATA","metadata":{}},{"cell_type":"code","source":"# Function to calculate missing values by column# Funct \ndef missing_values_table(weatherdata):\n        # Total missing values\n        mis_val = weatherdata.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * weatherdata.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"AIRPORT DATA","metadata":{}},{"cell_type":"code","source":"# Function to calculate missing values by column# Funct \ndef missing_values_table(airportdata):\n        # Total missing values\n        mis_val = airportdata.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * airportdata.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"summary statistics of the missing  values of the airpordata and weatherdata\n\n-The missing values will be analysed\n\n-All missing values with zero negligible influence number will be considered irrelevant","metadata":{}},{"cell_type":"code","source":"# Missing values statistics\nmissing_values = missing_values_table(weatherdata)\nmissing_values.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing values statistics\nmissing_values = missing_values_table(airportdata)\nmissing_values.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The airport data has zero missing values therefore it is safe to use uniformly for data analysis\n","metadata":{}},{"cell_type":"code","source":"Examine Missing Values\nNext we can look at the number and percentage of missing values in each column.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"airportdata.info()\nweatherdata.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"DATA CLEANING AND DATA SORTING\n#Check for empty columns and rows\n","metadata":{}},{"cell_type":"markdown","source":"AIRPORT DATA HOURS DISTRIBUTION","metadata":{}},{"cell_type":"code","source":"print(airportdata['hour_09_airport'].describe())\nplt.figure(figsize=(9, 8))\nsns.distplot(airportdata['hour_09_airport'], color='g', bins=100, hist_kws={'alpha': 0.4});","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(airportdata['hour_15_airport'].describe())\nplt.figure(figsize=(9, 8))\nsns.distplot(airportdata['hour_15_airport'], color='g', bins=100, hist_kws={'alpha': 0.4});","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Numerical data distribution\nFor this part lets look at the distribution of all of the features by ploting them\n\nTo do so lets first list all the types of our data from our dataset and take only the numerical ones:","metadata":{}},{"cell_type":"code","source":"list(set(weatherdata.dtypes.tolist()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_num2 = weatherdata.select_dtypes(include = ['float64', 'int64'])\ndf_num2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(set(airportdata.dtypes.tolist()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Therefore the above dataset values are all in numerical form","metadata":{}},{"cell_type":"code","source":"df_num1 = airportdata.select_dtypes(include = ['float64', 'int64'])\ndf_num1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PLOTTING OF ALL THE VARIABLES","metadata":{}},{"cell_type":"markdown","source":"AIRPORT DATA","metadata":{}},{"cell_type":"code","source":"df_num1.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8); # ; avoid having the matplotlib verbose informations","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"WEATHER DATA","metadata":{}},{"cell_type":"code","source":"df_num2.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8); # ; avoid having the matplotlib verbose informations","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Correlation\nNow we'll try to find which features are strongly correlated with SalePrice. We'll store them in a var called golden_features_list. We'll reuse our df_num dataset to do so.","metadata":{}},{"cell_type":"markdown","source":"Here are few suggestions for the advanced project:\n\n\n1.Make use of linear regression as a predictive model and improving it using polynomial regression. Find important features using RFE technique.\n\n2.Make use of various classification/prediction/clustering techniques from the unit\n\n3.Use various criteria (or metrics) for evaluation: e.g. use of Mean Square Error (MSE), Mean Absolute Error (MAE), and R-squared (r2) for regression problem. Use of accuracy, F-score, Area Under the ROC curve (AUC) for classification problem.\n\n4.First implement a simple algorithm (or model) as a baseline and then improving the baseline using more complex models/techniques.\n\n5.Do parameter analysis to find out which configuration of parameters give best model’s performance. For example, the performance under different k for the KNN algorithm.\n\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"Metric: ROC AUC\nOnce we have a grasp of the data (reading through the column descriptions helps immensely), we need to understand the metric by which our submission is judged. In this case, it is a common classification metric known as the Receiver Operating Characteristic Area Under the Curve (ROC AUC, also sometimes called AUROC).\n\nThe ROC AUC may sound intimidating, but it is relatively straightforward once you can get your head around the two individual concepts. The Reciever Operating Characteristic (ROC) curve graphs the true positive rate versus the false positive rate:\n\nimage\n\nA single line on the graph indicates the curve for a single model, and movement along a line indicates changing the threshold used for classifying a positive instance. The threshold starts at 0 in the upper right to and goes to 1 in the lower left. A curve that is to the left and above another curve indicates a better model. For example, the blue model is better than the red model, which is better than the black diagonal line which indicates a naive random guessing model.\n\nThe Area Under the Curve (AUC) explains itself by its name! It is simply the area under the ROC curve. (This is the integral of the curve.) This metric is between 0 and 1 with a better model scoring higher. A model that simply guesses at random will have an ROC AUC of 0.5.\n\nWhen we measure a classifier according to the ROC AUC, we do not generation 0 or 1 predictions, but rather a probability between 0 and 1. This may be confusing because we usually like to think in terms of accuracy, but when we get into problems with inbalanced classes (we will see this is the case), accuracy is not the best metric. For example, if I wanted to build a model that could detect terrorists with 99.9999% accuracy, I would simply make a model that predicted every single person was not a terrorist. Clearly, this would not be effective (the recall would be zero) and we use more advanced metrics such as ROC AUC or the F1 score to more accurately reflect the performance of a classifier. A model with a high ROC AUC will also have a high accuracy, but the ROC AUC is a better representation of model performance.\n\nNot that we know the background of the data we are using and the metric to maximize, let's get into exploring the data. In this notebook, as mentioned previously, we will stick to the main data sources and simple models which we can build upon in future work.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"4.1. Feature selection\n\n4.1.1. Recursive feature elimination\nGiven an external estimator that assigns weights to features, recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features.That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n\nReferences:","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\n​\ncols = [\"Age\",\"Fare\",\"TravelAlone\",\"Pclass_1\",\"Pclass_2\",\"Embarked_C\",\"Embarked_S\",\"Sex_male\",\"IsMinor\"] \nX = final_train[cols]\ny = final_train['Survived']\n# Build a logreg and compute the feature importances\nmodel = LogisticRegression()\n# create the RFE model and select 8 attributes\nrfe = RFE(model, 8)\nrfe = rfe.fit(X, y)\n# summarize the selection of the attributes\nprint('Selected features: %s' % list(X.columns[rfe.support_]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4.1.2. Feature ranking with recursive feature elimination and cross-validation\nRFECV performs RFE in a cross-validation loop to find the optimal number or the best number of features. Hereafter a recursive feature elimination applied on logistic regression with automatic tuning of the number of features selected with cross-validation.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import RFECV\n# Create the RFE object and compute a cross-validated score.\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nrfecv = RFECV(estimator=LogisticRegression(), step=1, cv=10, scoring='accuracy')\nrfecv.fit(X, y)\n\nprint(\"Optimal number of features: %d\" % rfecv.n_features_)\nprint('Selected features: %s' % list(X.columns[rfecv.support_]))\n\n# Plot number of features VS. cross-validation scores\nplt.figure(figsize=(10,6))\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Variables Kept","metadata":{}},{"cell_type":"code","source":"Selected_features = ['Age', 'TravelAlone', 'Pclass_1', 'Pclass_2', 'Embarked_C', \n                     'Embarked_S', 'Sex_male', 'IsMinor']\nX = final_train[Selected_features]\n\nplt.subplots(figsize=(8, 5))\nsns.heatmap(X.corr(), annot=True, cmap=\"RdYlGn\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4.2. Review of model evaluation procedures\nMotivation: Need a way to choose between machine learning models\n\nGoal is to estimate likely performance of a model on out-of-sample data\nInitial idea: Train and test on the same data\n\nBut, maximizing training accuracy rewards overly complex models which overfit the training data\nAlternative idea: Train/test split\n\nSplit the dataset into two pieces, so that the model can be trained and tested on different data\nTesting accuracy is a better estimate than training accuracy of out-of-sample performance\nProblem with train/test split\nIt provides a high variance estimate since changing which observations happen to be in the testing set can significantly change testing accuracy\nTesting accuracy can change a lot depending on a which observation happen to be in the testing set","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4.2.1. Model evaluation based on simple train/test split using train_test_split() function","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score \nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\n\n# create X (features) and y (response)\nX = final_train[Selected_features]\ny = final_train['Survived']\n\n# use train/test split with different random_state values\n# we can change the random_state values that changes the accuracy scores\n# the scores change a lot, this is why testing scores is a high-variance estimate\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n\n# check classification scores of logistic regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\ny_pred_proba = logreg.predict_proba(X_test)[:, 1]\n[fpr, tpr, thr] = roc_curve(y_test, y_pred_proba)\nprint('Train/Test split results:')\nprint(logreg.__class__.__name__+\" accuracy is %2.3f\" % accuracy_score(y_test, y_pred))\nprint(logreg.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_test, y_pred_proba))\nprint(logreg.__class__.__name__+\" auc is %2.3f\" % auc(fpr, tpr))\n\nidx = np.min(np.where(tpr > 0.95)) # index of the first threshold for which the sensibility > 0.95\n\nplt.figure()\nplt.plot(fpr, tpr, color='coral', label='ROC curve (area = %0.3f)' % auc(fpr, tpr))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot([0,fpr[idx]], [tpr[idx],tpr[idx]], 'k--', color='blue')\nplt.plot([fpr[idx],fpr[idx]], [0,tpr[idx]], 'k--', color='blue')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate (1 - specificity)', fontsize=14)\nplt.ylabel('True Positive Rate (recall)', fontsize=14)\nplt.title('Receiver operating characteristic (ROC) curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\nprint(\"Using a threshold of %.3f \" % thr[idx] + \"guarantees a sensitivity of %.3f \" % tpr[idx] +  \n      \"and a specificity of %.3f\" % (1-fpr[idx]) + \n      \", i.e. a false positive rate of %.2f%%.\" % (np.array(fpr[idx])*100))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"t4.2.3.\"></a>\n### 4.2.3. Model evaluation based on K-fold cross-validation using `cross_validate()` function ","metadata":{}},{"cell_type":"code","source":"# 10-fold cross-validation logistic regression\nlogreg = LogisticRegression()\n# Use cross_val_score function\n# We are passing the entirety of X and y, not X_train or y_train, it takes care of splitting the data\n# cv=10 for 10 folds\n# scoring = {'accuracy', 'neg_log_loss', 'roc_auc'} for evaluation metric - althought they are many\nscores_accuracy = cross_val_score(logreg, X, y, cv=10, scoring='accuracy')\nscores_log_loss = cross_val_score(logreg, X, y, cv=10, scoring='neg_log_loss')\nscores_auc = cross_val_score(logreg, X, y, cv=10, scoring='roc_auc')\nprint('K-fold cross-validation results:')\nprint(logreg.__class__.__name__+\" average accuracy is %2.3f\" % scores_accuracy.mean())\nprint(logreg.__class__.__name__+\" average log_loss is %2.3f\" % -scores_log_loss.mean())\nprint(logreg.__class__.__name__+\" average auc is %2.3f\" % scores_auc.mean())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_validate\n​\nscoring = {'accuracy': 'accuracy', 'log_loss': 'neg_log_loss', 'auc': 'roc_auc'}\n​\nmodelCV = LogisticRegression()\n​\nresults = cross_validate(modelCV, X, y, cv=10, scoring=list(scoring.values()), \n                         return_train_score=False)\n​\nprint('K-fold cross-validation results:')\nfor sc in range(len(scoring)):\n    print(modelCV.__class__.__name__+\" average %s: %.3f (+/-%.3f)\" % (list(scoring.keys())[sc], -results['test_%s' % list(scoring.values())[sc]].mean()\n                               if list(scoring.values())[sc]=='neg_log_loss' \n                               else results['test_%s' % list(scoring.values())[sc]].mean(), \n                               results['test_%s' % list(scoring.values())[sc]].std()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.3. GridSearchCV evaluating using multiple scorers simultaneously","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nX = final_train[Selected_features]\n\nparam_grid = {'C': np.arange(1e-05, 3, 0.1)}\nscoring = {'Accuracy': 'accuracy', 'AUC': 'roc_auc', 'Log_loss': 'neg_log_loss'}\n\ngs = GridSearchCV(LogisticRegression(), return_train_score=True,\n                  param_grid=param_grid, scoring=scoring, cv=10, refit='Accuracy')\n\ngs.fit(X, y)\nresults = gs.cv_results_\n\nprint('='*20)\nprint(\"best params: \" + str(gs.best_estimator_))\nprint(\"best params: \" + str(gs.best_params_))\nprint('best score:', gs.best_score_)\nprint('='*20)\n\nplt.figure(figsize=(10, 10))\nplt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\",fontsize=16)\n\nplt.xlabel(\"Inverse of regularization strength: C\")\nplt.ylabel(\"Score\")\nplt.grid()\n\nax = plt.axes()\nax.set_xlim(0, param_grid['C'].max()) \nax.set_ylim(0.35, 0.95)\n\n# Get the regular numpy array from the MaskedArray\nX_axis = np.array(results['param_C'].data, dtype=float)\n\nfor scorer, color in zip(list(scoring.keys()), ['g', 'k', 'b']): \n    for sample, style in (('train', '--'), ('test', '-')):\n        sample_score_mean = -results['mean_%s_%s' % (sample, scorer)] if scoring[scorer]=='neg_log_loss' else results['mean_%s_%s' % (sample, scorer)]\n        sample_score_std = results['std_%s_%s' % (sample, scorer)]\n        ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n                        sample_score_mean + sample_score_std,\n                        alpha=0.1 if sample == 'test' else 0, color=color)\n        ax.plot(X_axis, sample_score_mean, style, color=color,\n                alpha=1 if sample == 'test' else 0.7,\n                label=\"%s (%s)\" % (scorer, sample))\n\n    best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n    best_score = -results['mean_test_%s' % scorer][best_index] if scoring[scorer]=='neg_log_loss' else results['mean_test_%s' % scorer][best_index]\n        \n    # Plot a dotted vertical line at the best score for that scorer marked by x\n    ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n            linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n\n    # Annotate the best score for that scorer\n    ax.annotate(\"%0.2f\" % best_score,\n                (X_axis[best_index], best_score + 0.005))\n    ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n            linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n​\n    # Annotate the best score for that scorer\n    ax.annotate(\"%0.2f\" % best_score,\n                (X_axis[best_index], best_score + 0.005))\n\nplt.legend(loc=\"best\")\nplt.grid('off')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]}]}